{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d9cc3d",
   "metadata": {},
   "source": [
    "# ðŸŽµ Music Genre Classification using Neural Networks\n",
    "\n",
    "## Project Overview\n",
    "This project focuses on classifying music tracks into different genres using\n",
    "machine learning techniques. By extracting meaningful audio features from music\n",
    "files and training a neural network model, we aim to predict the genre of a given\n",
    "audio sample accurately.\n",
    "\n",
    "### Objectives\n",
    "- Extract relevant audio features from music files\n",
    "- Preprocess and normalize the dataset\n",
    "- Train a neural network for multi-class classification\n",
    "- Evaluate model performance using accuracy and confusion matrix\n",
    "- Predict the genre of unseen audio samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3397d6",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "This cell imports all necessary Python libraries used throughout the project:\n",
    "\n",
    "- **NumPy & Pandas**: data manipulation and numerical operations\n",
    "- **Librosa**: audio processing and feature extraction\n",
    "- **Scikit-learn**: preprocessing, encoding, and evaluation\n",
    "- **TensorFlow / Keras**: building and training the neural network\n",
    "- **Matplotlib & Seaborn**: data visualization\n",
    "\n",
    "These libraries together form a standard stack for audio-based machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "print(\"Libraries Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7220a99",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "The dataset contains extracted audio features from 30-second music clips.\n",
    "Each row represents one audio track, and each column corresponds to a specific\n",
    "audio feature or metadata.\n",
    "\n",
    "We load the dataset using Pandas to enable easy data exploration and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: Load and Clean the Data\n",
    "# Path to the dataset on Kaggle\n",
    "data_path = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/features_3_sec.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Drop columns that aren't 'features'\n",
    "# We drop 'filename' and 'length' (they don't help identify genre)\n",
    "data = df.drop(columns=['filename', 'length'])\n",
    "\n",
    "print(f\"Dataset Shape: {data.shape}\")\n",
    "data.head() # Shows the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d67aac",
   "metadata": {},
   "source": [
    "## Separate Features (X) and Labels (y)\n",
    "\n",
    "- **X** contains all numerical audio features\n",
    "- **y** contains the target variable: music genre\n",
    "\n",
    "Separating features and labels is a standard practice in supervised learning\n",
    "and prepares the data for encoding and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4795ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: Feature Scaling and Encoding\n",
    "\n",
    "# 1. Encode the Labels (Genres)\n",
    "genre_list = data.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(genre_list)\n",
    "\n",
    "# 2. Scale the Features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))\n",
    "\n",
    "# 3. Split into Training (80%) and Testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b922cc2",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "We construct a **fully connected neural network** with the following structure:\n",
    "\n",
    "- Input layer: audio feature vector\n",
    "- Hidden layers with ReLU activation for non-linearity\n",
    "- Dropout layers to reduce overfitting\n",
    "- Output layer with Softmax activation for multi-class classification\n",
    "\n",
    "This architecture allows the model to learn complex, non-linear patterns\n",
    "present in music audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d118227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: Building the Neural Network\n",
    "\n",
    "model = models.Sequential([\n",
    "    # Input Layer\n",
    "    layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Hidden Layers\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    # Output Layer (10 neurons for 10 genres)\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary() # This shows the \"map\" of your AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbfcf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Training the Model\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7333b",
   "metadata": {},
   "source": [
    "## Training and Validation Performance\n",
    "\n",
    "This cell visualizes:\n",
    "- Training accuracy vs. validation accuracy\n",
    "- Training loss vs. validation loss\n",
    "\n",
    "These plots help diagnose:\n",
    "- Overfitting\n",
    "- Underfitting\n",
    "- Training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b532004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Visualizing Results\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "plt.title('Accuracy Evolution')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Loss Evolution')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c58657b",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis\n",
    "\n",
    "The confusion matrix shows how well the model predicts each genre.\n",
    "It highlights:\n",
    "- Correct classifications (diagonal)\n",
    "- Misclassifications between similar genres\n",
    "\n",
    "This analysis helps identify which genres are most difficult\n",
    "for the model to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85391df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: The Confusion Matrix\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=encoder.classes_, \n",
    "            yticklabels=encoder.classes_)\n",
    "plt.title('Confusion Matrix: Predicted vs Actual Genres')\n",
    "plt.xlabel('Predicted Genre')\n",
    "plt.ylabel('Actual Genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a37f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre(audio_path):\n",
    "    # 1. Load the audio (first 3 seconds to match training)\n",
    "    y, sr = librosa.load(audio_path, duration=3)\n",
    "    \n",
    "    # 2. Extract MFCCs (The features our model expects)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    \n",
    "    # 3. Calculate the mean of these features across time\n",
    "    mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
    "    \n",
    "    # 4. We need to match the exact same number of features used in our training CSV\n",
    "    # The GTZAN CSV has 57 features (Chroma, RMS, Spectral Centroid, etc.)\n",
    "    # For a quick test, we will grab a random sample from our Test Set instead:\n",
    "    print(\"Processing audio features...\")\n",
    "\n",
    "# Let's pick a random song from your test set to see it in action!\n",
    "import random\n",
    "random_index = random.randint(0, len(X_test)-1)\n",
    "sample_to_test = X_test[random_index].reshape(1, -1)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(sample_to_test)\n",
    "predicted_index = np.argmax(prediction)\n",
    "actual_index = y_test[random_index]\n",
    "\n",
    "print(f\"\\n--- TEST RESULT ---\")\n",
    "print(f\"Model Prediction: {encoder.classes_[predicted_index]}\")\n",
    "print(f\"Actual Genre: {encoder.classes_[actual_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f020b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# Path to a random song in the dataset\n",
    "sample_song = \"/kaggle/input/samples/test_dj.mp3\"\n",
    "\n",
    "# Play it!\n",
    "print(\"Playing Sample...\")\n",
    "ipd.display(ipd.Audio(sample_song))\n",
    "\n",
    "# Note: In a real presentation, should run your prediction code right after this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9d58d",
   "metadata": {},
   "source": [
    "## Predict Genre for New Audio Samples\n",
    "\n",
    "This section demonstrates how to:\n",
    "1. Load a new audio file\n",
    "2. Extract the same audio features used during training\n",
    "3. Apply feature scaling\n",
    "4. Predict the music genre using the trained model\n",
    "\n",
    "This step simulates a real-world application of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def final_system_predict(file_path):\n",
    "    # 1. Load the audio\n",
    "    y, sr = librosa.load(file_path, offset=10, duration=3)\n",
    "    \n",
    "    # 2. Extract Basic Features\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    \n",
    "    # Use np.mean().item() to ensure we get a single float, not a numpy array\n",
    "    features = [\n",
    "        np.mean(chroma_stft).item(), np.var(chroma_stft).item(), \n",
    "        np.mean(rms).item(), np.var(rms).item(),\n",
    "        np.mean(spec_cent).item(), np.var(spec_cent).item(), \n",
    "        np.mean(spec_bw).item(), np.var(spec_bw).item(),\n",
    "        np.mean(rolloff).item(), np.var(rolloff).item(), \n",
    "        np.mean(zcr).item(), np.var(zcr).item()\n",
    "    ]\n",
    "    \n",
    "    # 3. Harmonic, Percussive, and Tempo\n",
    "    harmony, perceptr = librosa.effects.hpss(y)\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    \n",
    "    # Ensure tempo is a single float\n",
    "    if isinstance(tempo, np.ndarray):\n",
    "        tempo = tempo[0]\n",
    "\n",
    "    features.extend([\n",
    "        np.mean(harmony).item(), np.var(harmony).item(),\n",
    "        np.mean(perceptr).item(), np.var(perceptr).item(),\n",
    "        float(tempo) # Force tempo to be a single number\n",
    "    ])\n",
    "    \n",
    "    # 4. MFCCs (20 MFCCs * 2 = 40 features)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    for m in mfcc:\n",
    "        features.append(np.mean(m).item())\n",
    "        features.append(np.var(m).item())\n",
    "\n",
    "    # Check for consistency\n",
    "    print(f\"Features extracted: {len(features)}\") \n",
    "\n",
    "    # 5. Convert to clean Numpy array and Scale\n",
    "    features_np = np.array(features, dtype=float).reshape(1, -1)\n",
    "    features_scaled = scaler.transform(features_np) \n",
    "    \n",
    "    # 6. Predict\n",
    "    prediction = model.predict(features_scaled, verbose=0)\n",
    "    predicted_genre = encoder.classes_[np.argmax(prediction)]\n",
    "    confidence = np.max(prediction) * 100\n",
    "    \n",
    "   # print(f\"\\n--- RESULT: {predicted_genre.upper()} ({confidence:.2f}%) ---\")\n",
    "   # ipd.display(ipd.Audio(file_path))\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"SYSTEM ANALYSIS COMPLETE\")\n",
    "    print(f\"PREDICTED GENRE: {predicted_genre.upper()}\")\n",
    "    print(f\"CONFIDENCE: {confidence:.2f}%\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Play the audio so the professor can hear it\n",
    "    ipd.display(ipd.Audio(file_path))\n",
    "\n",
    "# TEST IT LIVE\n",
    "# You can use any file path from the dataset or your own uploaded .wav file\n",
    "my_song = \"/kaggle/input/samples/hiphop.mp3\"\n",
    "final_system_predict(my_song)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
